# Import all the needed libraries
import os
import zipfile
import requests
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score
import seaborn as sns
import matplotlib.pyplot as plt

# Download the dataset
dataset_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip"
zip_file_path = "UCI_HAR_Dataset.zip"
extracted_folder = "UCI_HAR_Dataset"

if not os.path.exists(zip_file_path):
    print("Downloading dataset...")
    response = requests.get(dataset_url, stream=True)
    with open(zip_file_path, "wb") as file:
        for chunk in response.iter_content(chunk_size=8192):
            file.write(chunk)
    print("Download complete.")

if not os.path.exists(extracted_folder):
    print("Extracting dataset...")
    with zipfile.ZipFile(zip_file_path, "r") as zip_ref:
        zip_ref.extractall(extracted_folder)
    print("Extraction complete.")

for root, dirs, files in os.walk(extracted_folder):
    if "features.txt" in files:
        dataset_folder = root
        break
print(f"Dataset found at: {dataset_folder}")

features_file = os.path.join(dataset_folder, "features.txt")
activity_labels_file = os.path.join(dataset_folder, "activity_labels.txt")

feature_list = pd.read_csv(features_file, sep=r"\s+", header=None, names=["index", "feature"])
feature_list["feature"] = feature_list["feature"].astype(str)  # Ensure string type

if feature_list["feature"].duplicated().any():
    feature_list["feature"] += "_" + feature_list["index"].astype(str)
feature_names = feature_list["feature"].tolist()

# activity labels
activity_labels = pd.read_csv(activity_labels_file, sep=r"\s+", header=None, names=["index", "activity"])
activity_mapping = dict(zip(activity_labels["index"], activity_labels["activity"]))

# Train the data
def load_dataset(split="train"):
    X_file = os.path.join(dataset_folder, split, f"X_{split}.txt")
    y_file = os.path.join(dataset_folder, split, f"y_{split}.txt")
    subject_file = os.path.join(dataset_folder, split, f"subject_{split}.txt")
   
    if not all(os.path.exists(f) for f in [X_file, y_file, subject_file]):
        raise FileNotFoundError(f"Missing files in {split} dataset.")
   
    X_data = pd.read_csv(X_file, sep=r"\s+", header=None, names=feature_names)
    y_data = pd.read_csv(y_file, sep=r"\s+", header=None, names=["Activity"])
    subjects = pd.read_csv(subject_file, sep=r"\s+", header=None, names=["Subject"])
   
    return X_data, y_data, subjects

# Load training and testing data
X_train, y_train, train_subjects = load_dataset("train")
X_test, y_test, test_subjects = load_dataset("test")

# Map activity labels to their names
y_train["Activity"] = y_train["Activity"].map(activity_mapping)
y_test["Activity"] = y_test["Activity"].map(activity_mapping)

# Binary Class
def map_to_binary(activity):
    return 1 if activity in ["WALKING", "WALKING_UPSTAIRS", "WALKING_DOWNSTAIRS"] else 0

y_train["Binary"] = y_train["Activity"].apply(map_to_binary)
y_test["Binary"] = y_test["Activity"].apply(map_to_binary)

y_train_binary = y_train["Binary"]
y_test_binary = y_test["Binary"]

# Feature scaling
feature_scaler = StandardScaler()
X_train_scaled = feature_scaler.fit_transform(X_train)
X_test_scaled = feature_scaler.transform(X_test)

# Grid seach
model_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=50)),  # Reduce features from 561 to 50
    ('svc', SVC(class_weight='balanced'))  # Handle class imbalance
])

# Hyperparameter for grid search
hyperparameter_grid = [
    {'svc__kernel': ['linear'], 'svc__C': [0.1, 1, 10, 100]},
    {'svc__kernel': ['poly'], 'svc__C': [0.1, 1], 'svc__degree': [2, 3], 'svc__gamma': [0.001, 0.01, 0.1]},
    {'svc__kernel': ['rbf'], 'svc__C': [0.1, 1, 10], 'svc__gamma': [0.001, 0.01, 0.1]}
]

# Perform grid search
grid_search = GridSearchCV(
    estimator=model_pipeline,
    param_grid=hyperparameter_grid,
    scoring='f1',  # Use F1 score for imbalanced datasets
    cv=3,
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_scaled, y_train_binary)

print("Best parameters:", grid_search.best_params_)
print("Best cross-validation F1 score:", grid_search.best_score_)

# Evaluating
y_pred = grid_search.predict(X_test_scaled)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test_binary, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Inactive', 'Active'],
            yticklabels=['Inactive', 'Active'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test_binary, y_pred, target_names=['Inactive', 'Active']))

# Additional Metrics
accuracy = accuracy_score(y_test_binary, y_pred)
f1 = f1_score(y_test_binary, y_pred)
precision = precision_score(y_test_binary, y_pred)
recall = recall_score(y_test_binary, y_pred)

print(f"Test Accuracy: {accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
